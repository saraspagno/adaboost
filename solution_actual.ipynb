{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10316d5779a3733",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise 2: Boosting\n",
    "\n",
    "## Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "\n",
    "* The homework assignments are executed automatically. \n",
    "* Failure to comply with the following instructions will result in a significant penalty. \n",
    "* Appeals regarding your failure to read these instructions will be denied. \n",
    "* Kind reminder: the homework assignments contribute 60% of the final grade.\n",
    "\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This Jupyter notebook contains all the step-by-step instructions needed for this exercise.\n",
    "1. Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise may take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit to this jupyter notebook. Tests will not be graded nor checked.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/).\n",
    "1. Your code must run without errors. Use at least `numpy` 1.15.4. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Submission includes a zip file that contains this notebook, with your ID as the file name. For example, `hw1_123456789_987654321.zip` if you submitted in pairs and `hw1_123456789.zip` if you submitted the exercise alone. The name of the notebook should follow the same structure.\n",
    "   \n",
    "Please use only a **zip** file in your submission.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions: \n",
    "### *** saraspagno@gmail.com,tirza.hanan@gmail.com ***\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735832cbfa43f83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c",
   "metadata": {},
   "source": [
    "# Part 1 - Design the AdaBoost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51651ffd",
   "metadata": {},
   "source": [
    "## 1.1 Theoretical Foundations\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is a sequential ensemble method that combines multiple **weak learners** into a single **strong classifier**.  \n",
    "A weak learner is a model that performs only slightly better than random guessing.\n",
    "\n",
    "Given training data $(x_i, y_i)$ with  \n",
    "$$\n",
    "y_i \\in \\{-1, +1\\},\n",
    "$$\n",
    "AdaBoost maintains a set of weights $w_i^{(t)}$ over training samples and trains a weak classifier at each boosting round.\n",
    "\n",
    "The final boosted classifier is:\n",
    "$$\n",
    "H_T(x) = \\sum_{t=1}^{T} \\alpha_t h_t(x),\n",
    "\\qquad\n",
    "\\hat{y}(x) = \\text{sign}(H_T(x)).\n",
    "$$\n",
    "\n",
    "AdaBoost minimizes the **exponential loss**:\n",
    "$$\n",
    "L(H) = \\sum_{i=1}^{n} \\exp\\big(-y_i H(x_i)\\big),\n",
    "$$\n",
    "which leads to a closed-form update rule for $\\alpha_t$.\n",
    "\n",
    "## 1.2 Practical Design Considerations\n",
    "\n",
    "### 1.2.1 Choice of Weak Learner\n",
    "\n",
    "Following the lecture guidelines, we use  \n",
    "$$\n",
    "\\text{DecisionTreeClassifier(max\\_depth = 1)}\n",
    "$$\n",
    "i.e., a **decision stump**.  \n",
    "\n",
    "Reasons:\n",
    "- Stumps precisely match the theoretical model analyzed in class.\n",
    "- They are fast, interpretable, and react strongly to weight changes.\n",
    "- They support weighted training using `sample_weight`.\n",
    "\n",
    "### 1.2.2 Weighted Training Strategy\n",
    "\n",
    "At iteration $t$, the weak learner is trained using the weight distribution:\n",
    "$$\n",
    "w^{(t)} = \\left(w_1^{(t)}, \\dots, w_n^{(t)}\\right),\n",
    "$$\n",
    "implemented through `sample_weight` in scikit-learn.\n",
    "\n",
    "This ensures examples with larger weights contribute more to the training objective.\n",
    "\n",
    "### 1.2.3 Label Representation\n",
    "\n",
    "AdaBoost is defined for labels $\\{-1,+1\\}$.  \n",
    "If the dataset uses $\\{0,1\\}$, we map:\n",
    "$$\n",
    "0 \\mapsto -1,\\qquad 1 \\mapsto +1.\n",
    "$$\n",
    "\n",
    "### 1.2.4 Numerical Stability Measures\n",
    "\n",
    "Practical precautions:\n",
    "- Weighted error is clipped:\n",
    "  $$\n",
    "  \\varepsilon_t \\leftarrow \\text{clip}(\\varepsilon_t, 10^{-12}, 1 - 10^{-12}).\n",
    "  $$\n",
    "- If $ \\varepsilon_t \\geq 0.5 $:  \n",
    "  the weak learner is not better than random \u2192 **stop early**.\n",
    "- If $ \\varepsilon_t = 0 $:  \n",
    "  the stump is perfectly accurate \u2192 boosting can terminate.\n",
    "\n",
    "These prevent exploding weights or division-by-zero.\n",
    "\n",
    "### 1.2.5 Stopping Criteria\n",
    "\n",
    "Boosting stops if any of these occurs:\n",
    "1. Perfect classifier is found.  \n",
    "2. Weak learner becomes no better than chance.  \n",
    "3. Maximum number of rounds $T$ is reached (hyperparameter).\n",
    "\n",
    "## 1.3 Full Algorithm Description\n",
    "\n",
    "### 1.3.1 Initialization\n",
    "\n",
    "$$\n",
    "w_i^{(1)} = \\frac{1}{n}, \\quad i = 1,\\dots,n.\n",
    "$$\n",
    "\n",
    "### 1.3.2 Boosting Iteration $t = 1, \\dots, T$\n",
    "\n",
    "**Step 1: Train weak classifier**\n",
    "\n",
    "Train a stump $h_t(x)$ using sample weights $w^{(t)}$.\n",
    "\n",
    "**Step 2: Compute weighted error**\n",
    "\n",
    "$$\n",
    "\\varepsilon_t = \\sum_{i=1}^{n} w_i^{(t)} \\cdot \\mathbf{1}[h_t(x_i) \\neq y_i].\n",
    "$$\n",
    "\n",
    "**Step 3: Compute learner weight**\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\ln\\left( \\frac{1 - \\varepsilon_t}{\\varepsilon_t} \\right).\n",
    "$$\n",
    "\n",
    "This weighs each learner by its accuracy.\n",
    "\n",
    "**Step 4: Update example weights**\n",
    "\n",
    "$$\n",
    "w_i^{(t+1)} = \\frac{w_i^{(t)} \\cdot \\exp(-\\alpha_t y_i h_t(x_i))}{Z_t},\n",
    "$$\n",
    "where $Z_t = \\sum_{i=1}^{n} w_i^{(t)} \\cdot \\exp(-\\alpha_t y_i h_t(x_i))$ is the normalization factor ensuring $\\sum_{i=1}^{n} w_i^{(t+1)} = 1$.\n",
    "\n",
    "Misclassified examples ($y_i h_t(x_i) < 0$) receive an exponential *increase* in weight.\n",
    "\n",
    "## 1.4 Hyperparameters\n",
    "\n",
    "| Hyperparameter | Meaning | Impact |\n",
    "|----------------|---------|--------|\n",
    "| **n\\_estimators (T)** | Maximum boosting rounds | Higher T \u2192 more expressive, but risk of overfitting |\n",
    "| **base\\_estimator** | Weak learner type | Stumps recommended for classical AdaBoost |\n",
    "| **random\\_state** | Reproducibility | Ensures deterministic experiments |\n",
    "| **error clipping** | Stability mechanism | Prevents numeric overflow |\n",
    "| **early stopping** | Stop when learner is no better than random | Avoids useless iterations |\n",
    "\n",
    "## 1.5 Limitations\n",
    "\n",
    "- **Highly sensitive to label noise**: due to exponential weight growth.  \n",
    "- **Uses exponential loss**: strongly penalizes misclassified samples (outliers).  \n",
    "- **Weak learners must be slightly better than random** ($\\varepsilon < 0.5$).  \n",
    "- **No built-in regularization**: unlike gradient boosting.  \n",
    "- **Performance deteriorates with many outliers or overlapping classes**.\n",
    "\n",
    "## 1.6 Use-Cases\n",
    "\n",
    "Suitable for:\n",
    "- Low-noise datasets.\n",
    "- Problems where simple rules (stumps) can partition the space.\n",
    "- Cases requiring interpretability.\n",
    "- Situations where speed and simplicity are important.\n",
    "- Medium-sized tabular datasets.\n",
    "\n",
    "Less suitable for:\n",
    "- High label-noise environments.\n",
    "- Very high-dimensional or sparse data.\n",
    "- Tasks requiring robustness to outliers.\n",
    "- Large-scale industrial datasets (gradient boosting is preferable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158ff2629daf2bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Part 2 - Implementing the AdaBoost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9102be37",
   "metadata": {},
   "source": [
    "## 2.1 Implementation Details\n",
    "\n",
    "Our implementation follows the AdaBoost.M1 procedure, with interface:\n",
    "\n",
    "- `__init__(self, T)`\n",
    "- `fit(self, X, y)`\n",
    "- `predict(self, X)`\n",
    "\n",
    "while allowing additional internal attributes for diagnostics and clarity.\n",
    "\n",
    "### **Initialization:**\n",
    "We store the required fields:\n",
    "\n",
    "- `self.T`: maximum number of boosting rounds  \n",
    "- `self.alphas`: weights of the weak learners  \n",
    "- `self.models`: list of trained decision stumps  \n",
    "\n",
    "Additionally, for analysis and debugging we maintain:\n",
    "\n",
    "- `self.epsilons`: weighted error at each round  \n",
    "- `self.weight_history`: full evolution of sample weights  \n",
    "- `self.Z_history`: normalization constants  \n",
    "\n",
    "### **Label Handling:**\n",
    "The code supports datasets with labels in `{0,1}` or `{-1,+1}`.  \n",
    "If `{0,1}` is detected, we convert them to `{-1,+1}` since AdaBoost\u2019s weight update uses signed labels.\n",
    "\n",
    "### **Base Weak Learner:**\n",
    "We follow the classical AdaBoost setup and use: `DecisionTreeClassifier(max_depth=1, random_state=42)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85d8f7447ebce0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class AdaBoostCustom:\n",
    "    def __init__(self, T):\n",
    "        \"\"\"\n",
    "        Custom implementation of AdaBoost.M1 using decision stumps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Maximum number of boosting rounds.\n",
    "        \"\"\"\n",
    "        self.T = T\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        self.epsilons = []\n",
    "        self.weight_history = []\n",
    "        self.Z_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit AdaBoost ensemble on data (X, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target labels in {0,1} or {-1,+1}.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : AdaBoostCustom\n",
    "            Returns self for method chaining.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        if set(np.unique(y)) == {0, 1}:\n",
    "            y = 2 * y - 1\n",
    "        elif set(np.unique(y)) != {-1, 1}:\n",
    "            raise ValueError(\"Labels must be in {0,1} or {-1,+1}.\")\n",
    "\n",
    "        n = len(X)\n",
    "        w = np.ones(n) / n\n",
    "        base_est = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "        self.alphas.clear()\n",
    "        self.models.clear()\n",
    "        self.epsilons.clear()\n",
    "        self.weight_history.clear()\n",
    "        self.Z_history.clear()\n",
    "\n",
    "        for t in range(self.T):\n",
    "            clf = clone(base_est)\n",
    "            clf.fit(X, y, sample_weight=w)\n",
    "            pred = clf.predict(X)\n",
    "\n",
    "            incorrect = (pred != y).astype(float)\n",
    "            eps = float(np.dot(w, incorrect))\n",
    "\n",
    "            eps = np.clip(eps, 1e-12, 1 - 1e-12)\n",
    "\n",
    "            if eps >= 0.5:\n",
    "                break\n",
    "\n",
    "            alpha = 0.5 * np.log((1 - eps) / eps)\n",
    "\n",
    "            exponent = -alpha * y * pred\n",
    "            exponent = np.clip(exponent, -700, 700)\n",
    "            w = w * np.exp(exponent)\n",
    "\n",
    "            Z = w.sum()\n",
    "            if Z <= 0 or not np.isfinite(Z):\n",
    "                break\n",
    "            w = w / Z\n",
    "\n",
    "            self.models.append(clf)\n",
    "            self.alphas.append(alpha)\n",
    "            self.epsilons.append(eps)\n",
    "            self.weight_history.append(w.copy())\n",
    "            self.Z_history.append(Z)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels for new samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input samples to predict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred : ndarray of shape (n_samples,)\n",
    "            Predicted labels in {-1,+1}.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        if len(self.models) == 0:\n",
    "            raise ValueError(\"Model has not been fitted yet.\")\n",
    "\n",
    "        margin = np.zeros(X.shape[0])\n",
    "        for alpha, clf in zip(self.alphas, self.models):\n",
    "            margin += alpha * clf.predict(X)\n",
    "\n",
    "        pred = np.sign(margin)\n",
    "        pred[pred == 0] = 1\n",
    "\n",
    "        return pred.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3129fb",
   "metadata": {},
   "source": [
    "# Part 3 - Demonstrate the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24f179351fa008",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Generate data\n",
    "Please use the cell below to discuss your dataset choice and why it is appropriate (or not) for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49bb42f79a55f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## AdaBoost demonstration \n",
    "Demonstrate your AdaBoost implementation.\n",
    "\n",
    "Add plots and figures. \n",
    "\n",
    "Please use the cell below to describe your results and tests.\n",
    "\n",
    "Describe the difference between your implementation and the sklearn implementation. Hint: you can look at the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508bd98d",
   "metadata": {},
   "source": [
    "# Part 4 - Experimental Design and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24247c8e",
   "metadata": {},
   "source": [
    "## Generate additional data sets\n",
    "Generate at least two experimental datasets with binary labels, designed to demonstrate specific properties of AdaBoost (e.g., handling noise or overfitting).\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below to describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11946a7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional data sets\n",
    "\n",
    "# Split data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3628856e1335fd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdaBoostClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m custom_model = AdaBoostCustom(T=\u001b[32m10\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sklearn_model = \u001b[43mAdaBoostClassifier\u001b[49m(n_estimators=\u001b[32m10\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Your code here\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'AdaBoostClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "custom_model = AdaBoostCustom(T=10)\n",
    "sklearn_model = AdaBoostClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5667598e-28fa-496e-b04b-ae528e653894",
   "metadata": {},
   "source": [
    "## Test Algorithms\n",
    "Test your AdaBoost, a library implementation of AdaBoost and at least two additional models, one of which must be another boosting algorithm on your two datasets.\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below to describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959a354-350d-49b8-b203-bdf889778766",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e4c3e86-003e-4812-8b36-65e13fccc4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d097c906-f362-4084-9ee8-0ff34568d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c95c7f-d3a9-4e3d-b539-02e020358766",
   "metadata": {},
   "source": [
    "# Part 5 - Use of generative AI\n",
    "Please use the cell below to describe your use of generative AI in this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}